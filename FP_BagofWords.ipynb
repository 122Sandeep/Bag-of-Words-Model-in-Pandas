{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model in Pandas\n",
    "\n",
    "A bag of words is a matrix representation of a document. It consists of several columns which are unique words. And every row is a new document. The cell values of every column indicate whether the word is present in the document or not. A dataframe representation is shown below. \n",
    "\n",
    "![Image](./data/bagofwords.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column (except doc_id) is a word. Each row is a new document. The first column is the name of the document. The first row is telling us that doc_id 1987_1 does not have the word abalone, abbeel or zhou. Hence each value is 0. If the word is contained in the document then that corresponding value in the column is 1.     \n",
    "\n",
    "We have to build this bag of words model with 5 documents.  The documents are named as doc1.txt, doc2.txt, doc3.txt, doc4.txt and doc5.txt.  \n",
    "\n",
    "**There should be 5 rows in the dataframe. The columns should be unique words in all documents. The columns should have words with length greater than 4. The words should not have any punctuation marks with it.**\n",
    "\n",
    "### From the DataFrame find the following information:\n",
    "1. Find out all the common words across the five documents.\n",
    "2. Find out all the uncommon words across the five documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo=open(\"./data/Bag of Words Docs/doc1.txt\",'r')\n",
    "text1 = fo.read()\n",
    "fo=open(\"./data/Bag of Words Docs/doc2.txt\",'r')\n",
    "text2 = fo.read()\n",
    "fo=open(\"./data/Bag of Words Docs/doc3.txt\",'r')\n",
    "text3 = fo.read()\n",
    "fo=open(\"./data/Bag of Words Docs/doc4.txt\",'r')\n",
    "text4 = fo.read()\n",
    "fo=open(\"./data/Bag of Words Docs/doc5.txt\",'r')\n",
    "text5 = fo.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data frame Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>100</th>\n",
       "      <th>academic</th>\n",
       "      <th>achieve</th>\n",
       "      <th>advances</th>\n",
       "      <th>ambiguity</th>\n",
       "      <th>amounts</th>\n",
       "      <th>analogies</th>\n",
       "      <th>analysis</th>\n",
       "      <th>appear</th>\n",
       "      <th>...</th>\n",
       "      <th>ways</th>\n",
       "      <th>won</th>\n",
       "      <th>word</th>\n",
       "      <th>word2vec</th>\n",
       "      <th>words</th>\n",
       "      <th>work</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  100  academic  achieve  advances  ambiguity  amounts  analogies  \\\n",
       "0    0    0         0        0         0          0        0          0   \n",
       "1    0    0         0        0         0          1        0          0   \n",
       "2    0    0         0        0         1          0        0          0   \n",
       "3    1    1         1        1         0          0        1          0   \n",
       "4    0    0         0        0         0          0        0          1   \n",
       "\n",
       "   analysis  appear  ...  ways  won  word  word2vec  words  work  works  \\\n",
       "0         2       0  ...     0    0     0         3      2     0      1   \n",
       "1         2       0  ...     0    0     0         1      1     0      0   \n",
       "2         0       0  ...     0    1     0         0      0     0      0   \n",
       "3         1       0  ...     1    0     0         1      0     1      0   \n",
       "4         0       1  ...     0    0     2         1      2     0      0   \n",
       "\n",
       "   world  years  york  \n",
       "0      0      0     0  \n",
       "1      0      0     0  \n",
       "2      0      1     1  \n",
       "3      0      0     0  \n",
       "4      1      0     0  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "sentence_1= text1\n",
    "sentence_2= text2\n",
    "sentence_3= text3\n",
    "sentence_4= text4\n",
    "sentence_5= text5\n",
    "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform([sentence_1,sentence_2,sentence_3,sentence_4,sentence_5])\n",
    "\n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
    "cv_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation  And the word length which are smaller than 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "text_file = [text1,text2,text3,text4,text5]\n",
    "st = [1,2,3,4,5,6]  #Only for creating the List\n",
    "stripped = [23,54,65,4,5,3]   #Only for creating the List\n",
    "for i in range(5):\n",
    "    st[i] = text_file[i]\n",
    "    string = [] \n",
    "    text = st[i].split(\" \") \n",
    "    for x in text: \n",
    "        if len(x) > 4: \n",
    "            string.append(x) \n",
    "    words = string\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped[i] = [w.translate(table) for w in words]\n",
    "\n",
    "str1 = stripped[0]\n",
    "str2 = stripped[1]\n",
    "str3 = stripped[2]\n",
    "str4 = stripped[3]\n",
    "str5 = stripped[4]\n",
    "\n",
    "#print(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tutorial competition little deeper sentiment analysis Googles Word2Vec deeplearning inspired method focuses meaning words Word2Vec attempts understand meaning semantic relationships among words works similar approaches recurrent neural neural nets computationally efficient tutorial focuses Word2Vec sentiment analysis\n"
     ]
    }
   ],
   "source": [
    "# Converting List into string\n",
    "Tolist = [1,2,3,4,5]\n",
    "for i in range(5):\n",
    "    strinn1 = \" \"\n",
    "    Tolist[i] = strinn1.join(stripped[i])\n",
    "\n",
    "print(Tolist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dictonary for each of the file for finding the frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doct1 = Tolist[0]\n",
    "doct2 = Tolist[1]\n",
    "doct3 = Tolist[2]\n",
    "doct4 = Tolist[3]\n",
    "doct5 = Tolist[4]\n",
    "du = [1,2,3,4,5] # For defainig the list\n",
    "dd = [doct1.split(),doct2.split(),doct3.split(),doct4.split(),doct5.split()]\n",
    "for i in range(5):\n",
    "    du[i] = dict()   \n",
    "    for line in dd[i]: \n",
    "        line = line.strip()  \n",
    "        line = line.lower() \n",
    "        words = line.split(\" \") \n",
    "        for word in words: \n",
    "            if word in du[i]: \n",
    "                du[i][word] = du[i][word] + 1\n",
    "            else: \n",
    "                du[i][word] = 1\n",
    "#print(du[1])\n",
    "#print(dd[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find out all the common words across the five documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "d1=du[0]\n",
    "d2=du[1]\n",
    "d3=du[2]\n",
    "d4=du[3]\n",
    "d5=du[4]\n",
    "c1= Counter(d1)\n",
    "c2= Counter(d2)\n",
    "c3= Counter(d3)\n",
    "c4=  Counter(d4)\n",
    "c5=  Counter(d5)\n",
    "t=c1&c2&c3&c4&c5\n",
    "t"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## As non of this word length is greater than 4 so there is no word which is common across the five documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Find out all the uncommon words across the five documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little', 'deeper', 'Googles', 'deeplearning', 'method', 'attempts', 'understand', 'semantic', 'among', 'works', 'recurrent', 'nets', 'computationally', 'efficient', 'Sentiment', 'challenging', 'subject', 'People', 'express', 'their', 'emotions', 'often', 'obscured', 'sarcasm', 'ambiguity', 'plays', 'could', 'misleading', 'humans', 'computers', 'Theres', 'another', 'review', 'explore', 'applied', 'problem', 'years', 'front', 'Times', 'These', 'techniques', 'architecture', 'human', 'brain', 'possible', 'recent', 'advances', 'computing', 'power', 'waves', 'breakthrough', 'results', 'speech', 'processing', 'natural', 'tasks', 'Recently', 'competitions', 'including', 'discovery', 'task', 'Since', 'rapidly', 'evolving', 'field', 'large', 'amounts', 'published', 'exists', 'academic', 'papers', 'exploratory', 'prescriptive', 'experiment', 'rather', 'giving', 'recipe', 'output\\n\\nTo', 'achieve', 'these', 'goals', '100000', 'multiparagraph', 'reviews', 'positive', 'negative', 'labels', 'order', 'create', 'meaningful', 'representations', 'useful', 'since', 'world', 'unlabeled', 'network', 'given', 'enough', 'training', 'tens', 'billions', 'produces', 'vectors', 'intriguing', 'characteristics', 'Words', 'meanings', 'appear', 'spaced', 'analogies', 'reproduced', 'vector', 'math']\n"
     ]
    }
   ],
   "source": [
    "def UncommonWords(A, B, C, D, E): \n",
    "  \n",
    "    count = {} \n",
    "    for word in A : \n",
    "        count[word] = count.get(word, 0) + 1\n",
    "      \n",
    "    for word in B: \n",
    "        count[word] = count.get(word, 0) + 1\n",
    "  \n",
    "    for word in C:\n",
    "        count[word] = count.get(word, 0) +1\n",
    "    \n",
    "    for word in D:\n",
    "        count[word] = count.get(word, 0) +1\n",
    "        \n",
    "    for word in E:\n",
    "        count[word] = count.get(word, 0) +1\n",
    "    # return required list of words \n",
    "    return [word for word in count if count[word] == 1] \n",
    "  \n",
    "A = str1\n",
    "B = str2\n",
    "C = str3\n",
    "D = str4\n",
    "E = str5\n",
    "print(UncommonWords(A, B, C,D,E)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
